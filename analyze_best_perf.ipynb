{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze the best performing models of each type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load modules\n",
    "import os\n",
    "import sys\n",
    "from time import time\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotnine\n",
    "from plotnine import *\n",
    "# Load the help functions\n",
    "from support.acc_funs import fast_auc, fast_decomp\n",
    "from support.support_funs import makeifnot, decomp_var\n",
    "\n",
    "from scipy.stats import rankdata\n",
    "from time import time\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "# Set directories\n",
    "dir_base = os.getcwd()\n",
    "dir_NSQIP = os.path.join(dir_base,'..')\n",
    "dir_output = os.path.join(dir_NSQIP, 'output')\n",
    "dir_figures = os.path.join(dir_NSQIP, 'figures')\n",
    "makeifnot(dir_figures)\n",
    "\n",
    "di_model = {'logit':'Logistic-L2', 'rf':'RandomForest', 'xgb':'XGBoost'}\n",
    "di_outcome = {'adv':'ADV', 'aki':'AKI', 'cns':'CNS',\n",
    "              'nsi':'nSSIs', 'ssi':'SSIs', 'unplan':'UPLN'}\n",
    "di_method = {'agg':'Aggregate', 'sub':'CPT-model'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dummy data to make sure fast approximations line up\n",
    "# y = np.array([1,1,1,0,0,0,0,0])\n",
    "# s = np.array([6,5,7,5,3,2,2,1])\n",
    "# g = np.array(['a','a','b','a','b','b','b','c'])\n",
    "# print(fast_decomp(y,s,g,ret_df=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading large CSV file\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>y</th>\n",
       "      <th>preds</th>\n",
       "      <th>cpt</th>\n",
       "      <th>test_year</th>\n",
       "      <th>outcome</th>\n",
       "      <th>method</th>\n",
       "      <th>version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>c24538</td>\n",
       "      <td>2016</td>\n",
       "      <td>nsi</td>\n",
       "      <td>agg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>c31541</td>\n",
       "      <td>2016</td>\n",
       "      <td>nsi</td>\n",
       "      <td>agg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model  y     preds     cpt  test_year outcome method  version\n",
       "0   xgb  0  0.000325  c24538       2016     nsi    agg        1\n",
       "1   xgb  0  0.000952  c31541       2016     nsi    agg        1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn_output = pd.Series(os.listdir(dir_output))\n",
    "fn_best = fn_output[fn_output.str.contains('.csv$') & fn_output.str.contains('^best')].reset_index(None,True)\n",
    "\n",
    "fn_csv = 'df_best.csv'\n",
    "check_csv = fn_csv in os.listdir(dir_output)\n",
    "if not check_csv:\n",
    "    print('Loading data via loop')\n",
    "    holder = []\n",
    "    cn_keep = ['model','outcome','test_year','cpt','y','preds']\n",
    "    for fn in fn_best:\n",
    "        print('Loading file: %s' % fn)\n",
    "        path = os.path.join(dir_output, fn)\n",
    "        tmp_df = pd.read_csv(path, usecols=cn_keep)  #, nrows=10\n",
    "        tmp_df.rename(columns={'model':'method'},inplace=True)\n",
    "        mdl = fn.split('.')[0].split('_')[-1]\n",
    "        tmp_df.insert(0,'model',mdl)\n",
    "        holder.append(tmp_df)\n",
    "        del tmp_df\n",
    "        #break\n",
    "    df_nsqip = pd.concat(holder).reset_index(None, True)\n",
    "    df_nsqip.outcome = df_nsqip.outcome.str.replace('agg_','')\n",
    "    df_nsqip['version'] = df_nsqip.outcome.str.replace('[^0-9]','')\n",
    "    df_nsqip.version = np.where(df_nsqip.version == '', '1', df_nsqip.version).astype(int)\n",
    "    df_nsqip.outcome = df_nsqip.outcome.str.replace('[^a-z]','')\n",
    "    del holder\n",
    "    print('Writing to file')\n",
    "    df_nsqip.to_csv(os.path.join(dir_output, fn_csv), index=False)\n",
    "else:\n",
    "    print('Loading large CSV file')\n",
    "    df_nsqip = pd.read_csv(os.path.join(dir_output, fn_csv))\n",
    "df_nsqip.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nsqip.groupby(['model','method']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Calculate within/between (all years and no years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FUNCTION TO TAKE IN RAW Y/PREDS SCORES AND DO THE DECOMPOSITION OVER ANY GROUP ORDER\n",
    "\"\"\"\n",
    "def write_fast_decomp(df, fn, cn, ret_df=False):\n",
    "    check = fn in os.listdir(dir_output)\n",
    "    if not check:\n",
    "        print('Running fast_decomp for %s' % fn)\n",
    "        stime = time()\n",
    "        tmp = df.groupby(cn).apply(lambda x: \n",
    "                   fast_decomp(x.y.values, x.preds.values, x.cpt.values, ret_df=ret_df))\n",
    "        print('Took %0.1f seconds to run decomp' % (time() - stime))\n",
    "        tmp = tmp.reset_index().drop(columns='level_'+str(len(cn)))\n",
    "        tmp.to_csv(os.path.join(dir_output, fn), index=False)\n",
    "    else:\n",
    "        print('Decomposition already exists, loading: %s' % fn)\n",
    "        tmp = pd.read_csv(os.path.join(dir_output, fn_within_year))\n",
    "    return tmp\n",
    "\n",
    "\"\"\"\n",
    "FUNCTION TO GENERATE FAST BOOTSTRAPS with fast_decomp:\n",
    "i) the within AUROC is a weighted sum of the CPT AUROCs\n",
    "ii) AFTER the CPT aurocs are generated, we can bootstrap these, rather than bootstrapping the data and then calculating\n",
    "\"\"\"\n",
    "\n",
    "# def bootstrap_within_decomp():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposition already exists, loading: df_within_year.csv\n",
      "Running fast_decomp for df_within.csv\n",
      "Took 49.0 seconds to run decomp\n"
     ]
    }
   ],
   "source": [
    "cn_gg1 = ['model','test_year','outcome','version','method']\n",
    "# Decompose including the year\n",
    "fn_within_year = 'df_within_year.csv'\n",
    "df_within_year = write_fast_decomp(df=df_nsqip, fn=fn_within_year, cn=cn_gg1, ret_df=False)\n",
    "\n",
    "# Decompose aggregating over years\n",
    "cn_gg2 = ['model','outcome','version','method']\n",
    "fn_within = 'df_within.csv'\n",
    "df_within = write_fast_decomp(df=df_nsqip, fn=fn_within, cn=cn_gg2, ret_df=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 49.0 seconds to run decomp\n"
     ]
    }
   ],
   "source": [
    "# USE AS THE BASIS FOR BOOTSTRAP WITHIN\n",
    "stime = time()\n",
    "qq = df_nsqip.groupby(cn_gg2).apply(lambda x: \n",
    "                   fast_decomp(x.y.values, x.preds.values, x.cpt.values, ret_df=True))\n",
    "print('Took %0.1f seconds to run decomp' % (time() - stime))\n",
    "qq = qq.reset_index().drop(columns='level_'+str(len(cn_gg2)))\n",
    "\n",
    "stime = time()\n",
    "for ii in range(100):\n",
    "    qq.groupby(cn_gg2).sample(frac=1,replace=True).groupby(cn_gg2).apply(lambda x: np.sum(x.auc*x.n0n1)/np.sum(x.n0n1))\n",
    "print('Took %0.1f seconds to run decomp' % (time() - stime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset to within\n",
    "df_decomp_within = df_decomp.query('tt == \"within\" & method==\"agg\"').reset_index(None, True).drop(columns=['tt','method'])\n",
    "# Find the best outcome/version\n",
    "best_decomp_all = df_decomp_within.groupby(['outcome','version']).auc.mean().reset_index()\n",
    "best_decomp_all = best_decomp_all.sort_values(['outcome','auc'],ascending=[True,False]).groupby('outcome').head(1)\n",
    "best_decomp_all = best_decomp_all.drop(columns='auc').reset_index(None,True)\n",
    "print(best_decomp_all)\n",
    "# Subset the decomposition\n",
    "df_decomp_within = df_decomp_within.merge(best_decomp_all, 'inner', ['outcome','version'])#.drop(columns='version')\n",
    "# Find the best within model for the best version\n",
    "best_decomp_mdl = df_decomp_within.groupby(['outcome','model']).apply(lambda x: np.sum(x.auc*x.den)/x.den.sum()).reset_index()\n",
    "best_decomp_mdl = best_decomp_mdl.rename(columns={0:'auc'}).sort_values(['outcome','auc'],ascending=[True,False])\n",
    "best_decomp_mdl = best_decomp_mdl.groupby(['outcome']).head(1).reset_index(None, True).drop(columns='auc')\n",
    "print(best_decomp_mdl)\n",
    "df_decomp_mdl = df_decomp.merge(best_decomp_all,'inner',['outcome','version'])#.drop(columns=['version'])\n",
    "df_decomp_mdl = df_decomp_mdl.merge(best_decomp_mdl,'inner',['outcome','model']).query('method==\"agg\"')\n",
    "df_decomp_mdl.drop(columns=['method','model'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.B) Inference on within/between AUROCs FOR BEST LABEL VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_inf_within = 'dat_decomp.csv'\n",
    "check_inf_within = fn_inf_within in os.listdir(dir_output)\n",
    "if not check_inf_within:\n",
    "    print('Running Bootstraps on within AUROC')\n",
    "    stime = time()\n",
    "    holder = []\n",
    "    n_bs = 1000\n",
    "    cn_drop = ['model','test_year','outcome','version','method']\n",
    "    cn_gg = ['cpt','n1','n0','n1n0']\n",
    "    squery = 'model==@model & test_year==@test_year & outcome==@outcome & version==@version & method==\"agg\"'\n",
    "    for ii, rr in dat_decomp_within.iterrows():\n",
    "        print('Row %i of %i' % (ii+1, dat_decomp_within.shape[0]))\n",
    "        model, test_year, outcome, version = rr['model'], rr['test_year'], rr['outcome'], rr['version']\n",
    "        tmp = df_nsqip.query(squery).copy()\n",
    "        # Keep only CPTs with a positive class\n",
    "        tmp2 = tmp.groupby('cpt').apply(lambda x: \n",
    "                pd.Series({'n1':sum(x.y==1),'n0':sum(x.y==0)})).reset_index()\n",
    "        tmp2 = tmp2.query('n1 > 0').reset_index(None, True)\n",
    "        tmp = tmp.merge(tmp2[['cpt']],'inner','cpt')\n",
    "        # Remove superfluous columns\n",
    "        tmp.drop(columns=cn_drop,inplace=True)\n",
    "        # Get the internal ranks\n",
    "        tmp = tmp.groupby('cpt').apply(lambda x: \n",
    "              pd.DataFrame({'y':x.y, 'preds':x.preds,'cpt':x.cpt,'r':rankdata(x.preds)})).sort_values('cpt')\n",
    "        tmp = tmp.query('y == 1').reset_index(None, True)\n",
    "        # ---- RUN VECTORIZED BOOTSTRAP ---- #\n",
    "        tmp_bs = tmp.sample(frac=n_bs,replace=True, random_state=ii).reset_index(None,True)\n",
    "        tmp_bs['idx'] = pd.Series(tmp_bs.index % n_bs).sample(frac=1, random_state=ii).values\n",
    "        tmp_bs = tmp_bs.groupby(['idx','cpt']).apply(lambda x: pd.Series({'r_s':x.r.sum(),'n1':len(x.r)})).reset_index()\n",
    "        tmp_bs = tmp_bs.merge(tmp2[['cpt','n0']]).assign(n1=lambda x: x.n1.astype(int), n1n0 = lambda x: x.n1 * x.n0)\n",
    "        tmp_bs = tmp_bs.assign(auc = lambda x: (x.r_s - x.n1*(x.n1+1)/2)/x.n1n0)\n",
    "        tmp_bs = tmp_bs.groupby('idx').apply(lambda x: np.sum(x.auc*x.n1n0)/np.sum(x.n1n0))\n",
    "        tmp_bs = tmp_bs.reset_index().rename(columns={0:'auc'})\n",
    "        tmp_bs = tmp_bs.assign(model=model, test_year=test_year, outcome=outcome, version=version)\n",
    "        holder.append(tmp_bs)\n",
    "        print(tmp_bs.head())\n",
    "        # Estimate the runtime\n",
    "        nsec, nleft = time() - stime, dat_decomp_within.shape[0] - (ii+1)\n",
    "        rate = (ii+1)/nsec\n",
    "        print('ETA: %0.1f minutes left' % (nleft / rate / 60))\n",
    "    dat_inf_within = pd.concat(holder)\n",
    "    dat_inf_within.to_csv(os.path.join(dir_output, fn_inf_within), index=False)\n",
    "else:\n",
    "    print('Loading existing file')\n",
    "    dat_inf_within = pd.read_csv(os.path.join(dir_output, fn_inf_within))\n",
    "# Use standard CI approach\n",
    "cv = stats.norm.ppf(0.975)\n",
    "cn_gg = ['model', 'test_year', 'outcome', 'version']\n",
    "alpha = 0.05\n",
    "cn_qq = [alpha/2, 1-alpha/2]\n",
    "tmp_inf = dat_inf_within.groupby(cn_gg).auc.quantile(cn_qq).reset_index()\n",
    "tmp_inf = tmp_inf.pivot_table('auc',cn_gg,'level_'+str(len(cn_gg))).rename(columns=dict(zip(cn_qq, ['lb','ub']))).reset_index()\n",
    "dat_decomp_within = dat_decomp_within.merge(tmp_inf)\n",
    "# tmp_inf = dat_inf_within.groupby(cn_gg).auc.std(ddof=1).reset_index().rename(columns={'auc':'se'})\n",
    "# dat_decomp_within = dat_decomp_within.merge(tmp_inf).assign(lb=lambda x: x.auc-cv*x.se, ub=lambda x: x.auc+cv*x.se)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.A) AUROC on the CPT level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_gg = ['model','outcome','version','method']  #,'test_year'\n",
    "\n",
    "fn_decomp_cpt = 'dat_decomp_cpt.csv'\n",
    "check_decomp_cpt = fn_decomp_cpt in os.listdir(dir_output)\n",
    "if not check_decomp_cpt:\n",
    "    print('Running decomposition for within')\n",
    "    stime = time()\n",
    "    dat_decomp_cpt = df_nsqip.groupby(cn_gg).apply(lambda x: \n",
    "                        fast_decomp(x.y.values, x.preds.values, x.cpt.values, ret_df=True))\n",
    "    dtime = time() - stime\n",
    "    print('Took %0.1f seconds to run decomp' % dtime)\n",
    "    dat_decomp_cpt = dat_decomp_cpt.reset_index()\n",
    "    dat_decomp_cpt = dat_decomp_cpt.drop(columns='level_'+str(len(cn_gg)))\n",
    "    dat_decomp_cpt.to_csv(os.path.join(dir_output, fn_decomp_cpt), index=False)\n",
    "else:\n",
    "    print('Loading decomposition by the cpt')\n",
    "    dat_decomp_cpt = pd.read_csv(os.path.join(dir_output, fn_decomp_cpt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg = ['model','outcome','version']\n",
    "# tmp_decomp = dat_decomp_cpt.merge(best_decomp_all, 'inner', ['outcome','version'])\n",
    "tmp_decomp = dat_decomp_cpt.query(\"method=='agg'\").groupby(gg).apply(lambda x: \n",
    "       pd.Series({'auc':x.auc.mean(), 'wauc':np.sum(x.auc*x.n0n1)/x.n0n1.sum(),'den':x.n0n1.sum()}))\n",
    "tmp_decomp = tmp_decomp.reset_index().melt(gg+['den'],None,'tt').assign(den=lambda x: x.den.astype(int))\n",
    "\n",
    "tmp_decomp2 = dat_decomp_within.groupby(gg).apply(lambda x: \n",
    "       pd.Series({'auc':x.auc.mean(), 'wauc':np.sum(x.auc*x.den)/x.den.sum(),'den':x.den.sum()}))\n",
    "tmp_decomp2 = tmp_decomp2.reset_index().melt(gg+['den'],None,'tt').assign(den=lambda x: x.den.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_decomp.merge(tmp_decomp2, 'inner',gg+['tt']).assign(\n",
    "    err_auc=lambda x: x.value_x - x.value_y, err_den=lambda x: x.den_x-x.den_y).head(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_decomp_within.query('model==\"logit\"&outcome==\"adv\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_decomp_cpt.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_decomp_cpt.query('g==\"c21175\"').sort_values('auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.B) Inference on the CPT level, aggregated over years for BEST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nsqip_cpt = df_nsqip.merge(best_decomp_all,'inner').merge(best_decomp_mdl,'inner').query('method==\"agg\"')\n",
    "df_nsqip_cpt = df_nsqip_cpt.reset_index(None,True).drop(columns=['method','model','version'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of counts over all years\n",
    "qq = df_nsqip_cpt.groupby(['outcome','cpt']).y.sum().reset_index()\n",
    "(ggplot(qq,aes(x='np.log(y+1)')) + geom_histogram(color='black',fill='red',alpha=0.5,bins=25) + \n",
    "theme_bw() + facet_wrap('~outcome',scales='free'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NUMBER OF CPTS PER OUTCOME/CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1 = dat_decomp_cpt.groupby(['model','test_year','outcome','method','g']).size().reset_index().drop(columns=0)\n",
    "tmp2 = tmp1.groupby(['model','test_year','outcome','method']).size().reset_index().rename(columns={0:'n'})\n",
    "tmp2.groupby('method').n.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.C) Merge with the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_boot = fn_output[fn_output.str.contains('boot_')].to_list()\n",
    "\n",
    "for fn in fn_boot[0:1]:\n",
    "    tmp = pd.read_csv(os.path.join(dir_output, fn),nrows=100)\n",
    "    print('------ %s ------' % fn)\n",
    "    print(tmp.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_sig = fn_output[fn_output.str.contains('sig_')].to_list()\n",
    "\n",
    "for fn in fn_sig[0:1]:\n",
    "    tmp = pd.read_csv(os.path.join(dir_output, fn),nrows=100)\n",
    "    print('------ %s ------' % fn)\n",
    "    print(tmp.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Decompose variation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.A) Variation between models\n",
    "\n",
    "There is little variation between models in performance. Exception for a few kidney models in some years. Also shows that for any given model, there is very little variation between years. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = dat_decomp_within.assign(model=lambda x: x.model.map(di_model)).copy()\n",
    "posd = position_dodge(0.5)\n",
    "w, h= 8, 4\n",
    "plotnine.options.figure_size = (w, h)\n",
    "gg_auc_within = (ggplot(tmp, aes(x='test_year.astype(str)',y='auc',color='model')) + \n",
    "                    theme_bw() + geom_point(position=posd) + \n",
    "                 geom_linerange(aes(ymin='lb',ymax='ub'),position=posd) + \n",
    "                facet_wrap('~outcome',labeller=labeller(outcome=di_outcome)) + \n",
    "                    theme(axis_text_x=element_text(angle=90)) + #,axis_ticks_minor_y=element_blank()\n",
    "                    labs(y='Within-CPT AUROC',x='Test year') + \n",
    "                scale_y_continuous(limits=[0.25,1],breaks=list(np.arange(0.25,1.01,0.25))) + \n",
    "                geom_hline(yintercept=0.5,linetype='--') + \n",
    "                scale_color_discrete(name='Model') + \n",
    "                ggtitle('Linerange shows 95% bootstrap CI'))\n",
    "gg_auc_within.save(os.path.join(dir_figures,'gg_auc_within.png'),base_height=h, base_width=w, verbose=False)\n",
    "gg_auc_within"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = dat_decomp_mdl.query('tt != \"total\"')\n",
    "w, h = 8, 4\n",
    "plotnine.options.figure_size = (w, h)\n",
    "gg_between = (ggplot(tmp,aes(x='test_year.astype(str)',y='auc',color='tt')) + \n",
    "              theme_bw() + geom_point(position=posd) + \n",
    "              facet_wrap('~outcome',labeller=labeller(outcome=di_outcome)) + \n",
    "             scale_color_discrete(name='AUC type',labels=['Between','Within']) + \n",
    "             labs(y='AUROC',x='Test year') + \n",
    "              geom_hline(yintercept=0.5,linetype='--') + \n",
    "             scale_y_continuous(limits=[0.25,1],breaks=list(np.arange(0.25,1.01,0.25))))\n",
    "gg_between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Use figures to visualize\n",
    "\n",
    "1. Largest variation between: between vs within AUROC\n",
    "2. The within AUROC has quite a lot of variation over CPT codes\n",
    "3. \\# of CPTs for a given year that make the cut-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.A) Within AUROC performance is similar acorss models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xlabs = [di_model[cc] for cc in dat_decomp_within_best.model.unique()]\n",
    "# gg_decomp_within = (ggplot(dat_decomp_within_best, aes(x='model',y='auc',color='model')) + \n",
    "#                     theme_bw() + geom_boxplot() + \n",
    "#                     facet_wrap('~outcome',labeller=labeller(outcome=di_outcome)) + \n",
    "#                     theme(legend_position='none',axis_title_x=element_blank(),axis_text_x=element_text(angle=90)) + \n",
    "#                     labs(y='Within-CPT AUROC') +\n",
    "#                     ggtitle('Within AUORIC variation over years/CPT-subset') + \n",
    "#                     scale_x_discrete(labels=xlabs))\n",
    "# h, w = 4, 8\n",
    "# gg_decomp_within.save(os.path.join(dir_figures,'gg_decomp_within.png'),base_height=h, base_width=w, verbose=False)\n",
    "# plotnine.options.figure_size = (w, h)\n",
    "# gg_decomp_within"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gg_decomp_mdl = (ggplot(dat_decomp_within_mdl, aes(x='outcome',y='auc',shape='model',color='test_year')) + \n",
    "#                     theme_bw() + geom_jitter(random_state=1, width=0.1, height=0) + \n",
    "#                     theme(axis_title_x=element_blank(),axis_text_x=element_text(angle=90)) + \n",
    "#                     labs(y='Within-CPT AUROC') +\n",
    "#                     ggtitle('Within AUORIC variation over years\\nOnly shows best performing model') + \n",
    "# #                 scale_color_discrete(name='Method',labels=list(di_method.values())) + \n",
    "#                 scale_shape_manual(name='Model', labels=list(di_model.values()), values=['$L$','$R$','$X$']) + \n",
    "#                 scale_x_discrete(labels=list(di_outcome.values())))\n",
    "# h, w = 4.5, 7\n",
    "# # gg_decomp_mdl.save(os.path.join(dir_figures,'gg_decomp_mdl.png'),base_height=h, base_width=w, verbose=False)\n",
    "# plotnine.options.figure_size = (w, h)\n",
    "# gg_decomp_mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# di_method = {'agg':'Aggregate', 'sub':'Subset'}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
